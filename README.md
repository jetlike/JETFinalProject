# ğŸ¤– Robot Vision & Voice MVP

This project enables a robot to recognize pointing gestures and respond to spoken questions like â€œHey Bot, what is this?â€

---

### ğŸ§  How It Works

1. **ğŸ‘‚ Voice Activation** â€” Listens for "Hey Bot" and records your question.
2. **ğŸ“ Transcription** â€” Converts your voice to text using Whisper.
3. **ğŸ–ï¸ Pointing Detection** â€” Detects your pointing gesture using MediaPipe.
4. **ğŸ¯ Object Detection** â€” Determines what you're pointing at.
5. **ğŸ¤– LLM Response** â€” Sends context to GPT-4 and gets a natural answer.
6. **ğŸ—£ï¸ Output** â€” Responds by text or voice.

---

### ğŸ“ Project Structure

robot-vision-voice-mvp/
â”œâ”€â”€ main.py               # Runs the full system
â”œâ”€â”€ requirements.txt      # Python dependencies
â”œâ”€â”€ README.md             # Overview and instructions
â”‚
â”œâ”€â”€ vision/
â”‚   â”œâ”€â”€ hand_tracker.py       # Hand + pointing gesture detection
â”‚   â””â”€â”€ object_detector.py    # Object detection from pointing
â”‚
â”œâ”€â”€ audio/
â”‚   â”œâ”€â”€ listener.py           # Wake word + audio recording
â”‚   â””â”€â”€ transcriber.py        # Speech-to-text with Whisper
â”‚
â”œâ”€â”€ llm/
â”‚   â””â”€â”€ query_engine.py       # Formats + sends LLM queries
â”‚
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ drawing.py            # Drawing helper functions
â”‚
â””â”€â”€ data/samples/         # Sample images/audio for testing
